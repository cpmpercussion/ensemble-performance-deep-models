{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "from urllib import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import permutations\n",
    "\n",
    "URL = \"https://github.com/anucc/metatone-analysis/raw/master/metadata/\"\n",
    "PICKLE_FILE = \"metatone_performances_dataframe.pickle\"\n",
    "\n",
    "if not os.path.exists(PICKLE_FILE):\n",
    "    urlretrieve(URL + PICKLE_FILE, PICKLE_FILE)\n",
    "\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "        metatone_dataset = pickle.load(f)\n",
    "        \n",
    "## Int values for Gesture codes.\n",
    "NUMBER_GESTURES = 9\n",
    "GESTURE_CODES = {\n",
    "    'N': 0,\n",
    "    'FT': 1,\n",
    "    'ST': 2,\n",
    "    'FS': 3,\n",
    "    'FSA': 4,\n",
    "    'VSS': 5,\n",
    "    'BS': 6,\n",
    "    'SS': 7,\n",
    "    'C': 8}\n",
    "\n",
    "vocabulary_size = len(GESTURE_CODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encodes (and decodes) multiple metatone performance gesture codes into single natural numbers.\n",
    "Given n gesture codes g_1,g_2,\\ldots,g_n in the range [1,j-1], these can be encoded as a unique integer:\n",
    "g_1j^0 + g_2j^1 + \\ldots + g_nj^(n-1)\n",
    "And subsequently decoded into the original ordered set.\n",
    "\"\"\"\n",
    "\n",
    "# Int values for Gesture codes.\n",
    "NUMBER_GESTURES = 9\n",
    "GESTURE_CODES = {\n",
    "    'N': 0,\n",
    "    'FT': 1,\n",
    "    'ST': 2,\n",
    "    'FS': 3,\n",
    "    'FSA': 4,\n",
    "    'VSS': 5,\n",
    "    'BS': 6,\n",
    "    'SS': 7,\n",
    "    'C': 8}\n",
    "\n",
    "\n",
    "def encode_ensemble_gestures(gestures):\n",
    "    \"\"\"Encode multiple natural numbers into one\"\"\"\n",
    "    encoded = 0\n",
    "    for i, g in enumerate(gestures):\n",
    "        encoded += g * (len(GESTURE_CODES) ** i)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_ensemble_gestures(num_perfs, code):\n",
    "    \"\"\"Decodes ensemble gestures from a single int\"\"\"\n",
    "    # TODO: Check that this works correctly now.\n",
    "    gestures = []\n",
    "    for i in range(num_perfs):\n",
    "        part = code % (len(GESTURE_CODES) ** (i + 1))\n",
    "        gestures.append(part // (len(GESTURE_CODES) ** i))\n",
    "    return gestures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolate Individual Performances\n",
    "improvisations = metatone_dataset[\n",
    "    (metatone_dataset[\"performance_type\"] == \"improvisation\") &\n",
    "    (metatone_dataset[\"performance_context\"] != \"demonstration\") &\n",
    "    (metatone_dataset[\"number_performers\"] != 4)]\n",
    "gesture_data = improvisations['gestures']\n",
    "individual_improvisations = []\n",
    "for perf in gesture_data.tolist():\n",
    "    for one_perf in perf.T:\n",
    "        individual_improvisations.append(one_perf)\n",
    "print(\"Number of performances for testing: \", len(individual_improvisations))\n",
    "        \n",
    "## Isolate the Interesting Ensemble Performances\n",
    "improvisations = metatone_dataset[\n",
    "    (metatone_dataset[\"performance_type\"] == \"improvisation\") &\n",
    "    (metatone_dataset[\"performance_context\"] != \"demonstration\") &\n",
    "    (metatone_dataset[\"number_performers\"] == 4)]\n",
    "gesture_data = improvisations['gestures']\n",
    "#metatone_dataset[\"number_performers\"]\n",
    "\n",
    "num_input_performers = 4\n",
    "num_output_performers = 3\n",
    "\n",
    "ensemble_improvisations = gesture_data.tolist()\n",
    "\n",
    "## Setup the epochs\n",
    "## Each batch is of single gestures as input and tuples of remaining performers as output\n",
    "def generate_epochs(num_epochs, num_steps, batch_size):\n",
    "    ## Setup the inputs and label sets\n",
    "    imp_xs = []\n",
    "    imp_ys = []\n",
    "    \n",
    "    for imp in ensemble_improvisations:\n",
    "        for i in range(len(imp)-num_steps-1):\n",
    "            imp_slice = imp[i:i+num_steps+1]\n",
    "            for j in range(len(imp_slice.T)):\n",
    "                lead = imp_slice[1:].T[j] # lead gestures (post steps)\n",
    "                ensemble = imp_slice.T[np.arange(len(imp_slice.T)) != j] # rest of the players indexed by player\n",
    "                for ens_perm in permutations(ensemble): # consider all permutations of the players\n",
    "                    ens_pre = np.array(ens_perm).T[:-1] # indexed by time slice\n",
    "                    ens_post = np.array(ens_perm).T[1:] # indexed by time slice\n",
    "                    y = map(encode_ensemble_gestures,ens_post)\n",
    "                    #y = ens_post # test just show the gestures\n",
    "                    x = map(encode_ensemble_gestures,zip(lead,*(ens_pre.T))) # encode ensemble state\n",
    "                    #x = zip(lead,*(ens_pre.T)) # test just show the gestures\n",
    "                    imp_xs.append(x) # append the inputs\n",
    "                    imp_ys.append(y) # append the outputs\n",
    "    dataset = zip(imp_xs,imp_ys)\n",
    "    print(\"Total Training Examples: \" + str(len(imp_xs)))\n",
    "    print(\"Total Training Labels: \" + str(len(imp_ys)))\n",
    "    epochs = []\n",
    "    for j in range(num_epochs):\n",
    "        # shutffle the big list\n",
    "        np.random.shuffle(dataset)\n",
    "        dataset_size = len(dataset)\n",
    "        batches = []\n",
    "        for i in range(dataset_size / batch_size):\n",
    "            ## Setup the batches\n",
    "            batch = dataset[i*batch_size:(i+1)*batch_size]\n",
    "            bx,by = zip(*batch)\n",
    "            batches.append((np.array(bx),np.array(by)))\n",
    "        epochs.append(batches)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "num_steps = 120\n",
    "\n",
    "examples_file = \"../MetatoneQuartetExamples-\" + str(num_steps) + \"steps\" + \".h5\"\n",
    "\n",
    "with h5py.File(examples_file, 'r') as data_file:\n",
    "        dataset = data_file['examples'][:]\n",
    "        validation_set = data_file['validation'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (343482, 120)\n",
      "y shape: (343482, 1)\n"
     ]
    }
   ],
   "source": [
    "# dataset file has examples of shape (2, num_steps), \n",
    "#where the first row is the input, and second \n",
    "# is the correct output sequence.\n",
    "# First step is to adapt it into many-to-one format\n",
    "X = dataset[:,0,:] # take all the input sequences\n",
    "y = dataset[:,1,-1] # just select the last one of each output sequence\n",
    "y = y.reshape((-1,1)) # reshape to (n,1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 120, 32)           209952    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 729)               24057     \n",
      "=================================================================\n",
      "Total params: 242,329\n",
      "Trainable params: 242,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of input classes: 6561\n",
      "Number of output classes 729\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "batch_size = 64\n",
    "num_units = 32\n",
    "num_steps = 120\n",
    "\n",
    "num_input_performers = 4\n",
    "num_output_performers = 3\n",
    "\n",
    "vocabulary_size = 9 # len(GESTURE_CODES)\n",
    "num_input_classes = vocabulary_size ** num_input_performers\n",
    "num_output_classes = vocabulary_size ** num_output_performers\n",
    "\n",
    "training_model = keras.models.Sequential()\n",
    "training_model.add(keras.layers.Embedding(num_input_classes, num_units, input_length=num_steps))\n",
    "for n in range(num_layers - 1):\n",
    "    training_model.add(keras.layers.LSTM(num_units, return_sequences=True))\n",
    "training_model.add(keras.layers.LSTM(num_units))\n",
    "training_model.add(keras.layers.Dense(num_output_classes, activation='softmax'))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "training_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam')\n",
    "training_model.summary()\n",
    "\n",
    "# Notes:\n",
    "# Difficulty of this task is learning from a relatively large input space:\n",
    "print(\"Number of input classes:\", num_input_classes)\n",
    "print(\"Number of output classes\", num_output_classes)\n",
    "# It's handy to use an Embedding layer so that we can learn from integer\n",
    "# inputs (not one-hot)\n",
    "# This means that for lower 'num_units', the parameters used for the input \n",
    "# embedding outnumber the LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 291959 samples, validate on 51523 samples\n",
      "Epoch 1/1\n",
      " 75328/291959 [======>.......................] - ETA: 5:12 - loss: 5.1939"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3d74ea233434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_model.fit(X, y, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
