{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative Machine Learning Experiments on the Metatone Data\n",
    "\n",
    "In this notebook, I'll work on some ML experiments with the Metatone performance data. The goal is to create models of these performances that are more realistic than the first order models used in earlier experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn import datasets, metrics, cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "URL = \"https://github.com/anucc/metatone-analysis/raw/master/metadata/\"\n",
    "PICKLE_FILE = \"metatone_performances_dataframe.pickle\"\n",
    "\n",
    "if not os.path.exists(PICKLE_FILE):\n",
    "    urlretrieve(URL + PICKLE_FILE, PICKLE_FILE)\n",
    "\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "        metatone_dataset = pickle.load(f)\n",
    "        \n",
    "## Int values for Gesture codes.\n",
    "NUMBER_GESTURES = 9\n",
    "GESTURE_CODES = {\n",
    "    'N': 0,\n",
    "    'FT': 1,\n",
    "    'ST': 2,\n",
    "    'FS': 3,\n",
    "    'FSA': 4,\n",
    "    'VSS': 5,\n",
    "    'BS': 6,\n",
    "    'SS': 7,\n",
    "    'C': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 72\n",
      "Min performers: 2 Max performers: 9\n",
      "Total performer-records: 252\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFkCAYAAAA0bNKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOTEHgM79X3EmLYwed5rKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112ea5f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improvisations = metatone_dataset[\n",
    "    (metatone_dataset[\"performance_type\"] == \"improvisation\") &\n",
    "    (metatone_dataset[\"performance_context\"] != \"demonstration\")]\n",
    "\n",
    "#print(str(improvisations[\"performance_type\"].unique()))\n",
    "#print(str(improvisations[\"performance_context\"].unique()))\n",
    "\n",
    "print(\"Number of records: \" + str(improvisations[\"number_performers\"].count()))\n",
    "print(\"Min performers: \" + str(improvisations[\"number_performers\"].min()) + \" Max performers: \"+ str(improvisations[\"number_performers\"].max()))\n",
    "print(\"Total performer-records: \" + str(improvisations[\"number_performers\"].sum()))\n",
    "\n",
    "\n",
    "performers = improvisations[\"number_performers\"].plot(kind=\"box\")\n",
    "plt.show(performers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "Isolate the gesture arrays and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 252 individual improvisations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesture_data = improvisations['gestures']\n",
    "individual_improvisations = []\n",
    "\n",
    "\n",
    "for perf in gesture_data.tolist():\n",
    "    for one_perf in perf.T:\n",
    "        individual_improvisations.append(one_perf)\n",
    "\n",
    "print(\"Dataset contains \" + str(len(individual_improvisations)) + \" individual improvisations.\")\n",
    "\n",
    "vocabulary_size = len(GESTURE_CODES)\n",
    "\n",
    "def code_to_one_hot(code):\n",
    "    \n",
    "\n",
    "\n",
    "#individual_improvisations\n",
    "#a = gesture_data.tolist()        \n",
    "#b = a[0]\n",
    "#for c in b.T:\n",
    "#    print(c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment and Prepare the Data\n",
    "\n",
    "Segment the data into inputs and output list of length `num_steps`. Each record in the data is sliced into as many such lists as possible and the total size of the corpus is printed out for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Inputs: 105547\n",
      "Training Labels: 105547\n"
     ]
    }
   ],
   "source": [
    "## Setup the inputs and label sets\n",
    "num_steps = 100\n",
    "\n",
    "imp_xs = []\n",
    "imp_ys = []\n",
    "\n",
    "for imp in individual_improvisations:\n",
    "    for i in range(len(imp)-num_steps-1):\n",
    "        imp_x = imp[i:i+num_steps]\n",
    "        imp_y = imp[i+1:i+num_steps+1]\n",
    "        imp_xs.append(imp_x)\n",
    "        imp_ys.append(imp_y)\n",
    "\n",
    "print(\"Total Training Examples: \" + str(len(imp_xs)))\n",
    "print(\"Total Training Labels: \" + str(len(imp_ys)))\n",
    "\n",
    "## Setup the batches\n",
    "\n",
    "## Setup the epochs\n",
    "\n",
    "generate_epochs(num_epochs = 30, num_steps = 100, batch_size = 128):\n",
    "    # todo fill in this function.\n",
    "    return none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM Model\n",
    "\n",
    "Setup a simple 3-Layer LSTM Model with 64 nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "num_classes = vocabulary_size\n",
    "batch_size = 64\n",
    "num_steps = 200\n",
    "num_layers = 3\n",
    "learning_rate = 1e-4\n",
    "  \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.int32,[batch_size,num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32,[batch_size,num_steps], name='labels_placeholder')\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, num_nodes])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings,x)\n",
    "    \n",
    "    # Define the network\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_nodes,state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size,tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W',[num_nodes,num_classes])\n",
    "        b = tf.get_variable('b',[num_classes], initializer=tf.constant_initializer(0.0))\n",
    "       \n",
    "    rnn_outputs = tf.reshape(rnn_outputs,[-1,num_nodes])\n",
    "    y_reshaped = tf.reshape(y,[-1])\n",
    "    \n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_placeholder:0' shape=(64, 200) dtype=int32>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'labels_placeholder:0' shape=(64, 200) dtype=int32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_epochs(num_epochs, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "tf.set_random_seed(2345)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in enumerate(generate_epochs(num_epochs, num_steps, batch_size)):\n",
    "        ## Setup for each epoch\n",
    "        training_loss = 0\n",
    "        steps = 0\n",
    "        training_state = None\n",
    "        for batch_data, batch_labels in epoch:\n",
    "            ## Setup for each batch\n",
    "            feed_dict = {x:batch_data, y:batch_labels}\n",
    "            if training_state is not None:\n",
    "                feed_dict[init_state] = training_state\n",
    "            ## Evaluate the train step\n",
    "            training_loss_current, training_state, _ = sess.run(total_loss,final_state,train_step,feed_dict=feed_dict)\n",
    "            steps += 1\n",
    "            training_loss += training_loss_current\n",
    "        print(\"Trained \")\n",
    "        training_losses.append(training_loss/steps)\n",
    "    ## Save the results\n",
    "    tf.train.Saver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
